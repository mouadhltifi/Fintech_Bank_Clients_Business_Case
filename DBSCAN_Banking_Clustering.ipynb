{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e5fde0",
   "metadata": {},
   "source": [
    "# **DBSCAN Clustering for Banking Clients (Fixed Version)**\n",
    "\n",
    "This notebook applies **DBSCAN clustering** on a **banking client dataset**, using **Gower distance** to handle mixed numerical & categorical data. \n",
    "\n",
    "### **üîπ Pipeline Overview:**\n",
    "1. **Load & Preprocess Data** (Handling categorical & numerical features)\n",
    "2. **Compute Gower Distance** (for mixed numerical & categorical data)\n",
    "3. **Find Optimal `eps`** Using a K-Distance Graph (Elbow Method)\n",
    "4. **Auto-Tune DBSCAN** to find a balanced number of clusters\n",
    "5. **Evaluate Clustering Performance** (Silhouette, Davies-Bouldin, Calinski-Harabasz Scores)\n",
    "6. **Visualize Results using t-SNE in 3D**\n",
    "7. **Interpret the Clusters** (Restoring numerical values & categorical labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd906ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load dataset (Update path if necessary)\n",
    "path = '/Users/mouadh/Fintech_Projects/Business_Case_1/Dataset1_BankClients.xlsx'\n",
    "data = pd.read_excel(path)\n",
    "\n",
    "# Drop ID column if present\n",
    "if 'ID' in data.columns:\n",
    "    data = data.drop(columns=['ID'])\n",
    "\n",
    "# Display first 5 rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83667c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_columns = ['Gender', 'Job', 'Area', 'CitySize', 'Investments']\n",
    "\n",
    "# Separate numerical & categorical features\n",
    "numerical_features = data.drop(columns=categorical_columns)\n",
    "categorical_features = data[categorical_columns]\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = MinMaxScaler()\n",
    "X_num_scaled = scaler.fit_transform(numerical_features)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "X_cat_encoded = encoder.fit_transform(categorical_features)\n",
    "\n",
    "# Concatenate processed features\n",
    "X_preprocessed = np.concatenate((X_num_scaled, X_cat_encoded), axis=1)\n",
    "\n",
    "# Save feature names\n",
    "encoded_feature_names = encoder.get_feature_names_out(categorical_columns)\n",
    "all_feature_names = numerical_features.columns.tolist() + encoded_feature_names.tolist()\n",
    "\n",
    "# Check final shape\n",
    "print(f'Processed Data Shape: {X_preprocessed.shape}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d1473",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gower\n",
    "\n",
    "# Compute Gower Distance Matrix\n",
    "gower_distances = gower.gower_matrix(X_preprocessed)\n",
    "\n",
    "# Display first 5 rows\n",
    "gower_distances[:5, :5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb31a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to find optimal eps using k-distance graph\n",
    "def find_optimal_eps(distance_matrix, k=8, percentile_range=(90, 98)):\n",
    "    nbrs = NearestNeighbors(n_neighbors=k, metric='precomputed').fit(distance_matrix)\n",
    "    distances, indices = nbrs.kneighbors(distance_matrix)\n",
    "\n",
    "    sorted_distances = np.sort(distances[:, -1], axis=0)\n",
    "\n",
    "    # Plot k-distance graph\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(sorted_distances)\n",
    "    plt.xlabel(\"Points sorted by distance\")\n",
    "    plt.ylabel(f\"Distance to {k}-th nearest neighbor\")\n",
    "    plt.title(\"K-Distance Graph (Use Elbow Method for eps)\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    for p in range(percentile_range[0], percentile_range[1] + 1, 2):\n",
    "        test_eps = np.percentile(sorted_distances, p)\n",
    "        print(f\"üîπ Trying eps at {p}th percentile: {test_eps:.4f}\")\n",
    "        if 0.05 <= test_eps <= 0.2:\n",
    "            return test_eps\n",
    "\n",
    "    return np.percentile(sorted_distances, 95)\n",
    "\n",
    "# Automatically determine initial eps\n",
    "optimal_eps = find_optimal_eps(gower_distances, k=8, percentile_range=(90, 98))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0081b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "# Automatically determine min_samples\n",
    "min_samples_range = (3, 12)\n",
    "optimal_min_samples = max(int(np.log(len(gower_distances))), min_samples_range[0])\n",
    "optimal_min_samples = min(optimal_min_samples, min_samples_range[1])\n",
    "\n",
    "print(f\"‚úÖ Initial min_samples: {optimal_min_samples}\")\n",
    "\n",
    "# Apply DBSCAN clustering\n",
    "dbscan = DBSCAN(eps=optimal_eps, min_samples=optimal_min_samples, metric='precomputed')\n",
    "labels_dbscan = dbscan.fit_predict(gower_distances)\n",
    "\n",
    "unique_clusters = np.unique(labels_dbscan)\n",
    "num_clusters = len(unique_clusters) - (1 if -1 in unique_clusters else 0)\n",
    "num_noise = sum(labels_dbscan == -1)\n",
    "\n",
    "print(f\"‚úÖ Final eps selected: {optimal_eps:.4f}\")\n",
    "print(f\"‚úÖ Final min_samples: {optimal_min_samples}\")\n",
    "print(f\"‚úÖ Final clusters: {num_clusters}\")\n",
    "print(f\"‚úÖ Final noise points: {num_noise}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347b2170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score\n",
    "\n",
    "if num_clusters > 1:\n",
    "    ch_score = calinski_harabasz_score(X_preprocessed, labels_dbscan)\n",
    "    db_score = davies_bouldin_score(X_preprocessed, labels_dbscan)\n",
    "    sil_score = silhouette_score(gower_distances, labels_dbscan, metric='precomputed')\n",
    "\n",
    "    print(\"üîπ **Cluster Evaluation Metrics**:\")\n",
    "    print(f\"‚úÖ **Calinski-Harabasz Score:** {ch_score:.4f} (Higher = Better)\")\n",
    "    print(f\"‚úÖ **Davies-Bouldin Score:** {db_score:.4f} (Lower = Better)\")\n",
    "    print(f\"‚úÖ **Silhouette Score:** {sil_score:.4f} (Closer to 1 = Better)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Not enough clusters detected for evaluation.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back to DataFrame with proper labels\n",
    "cluster_summary = pd.DataFrame(X_preprocessed, columns=all_feature_names)\n",
    "cluster_summary['Cluster'] = labels_dbscan\n",
    "\n",
    "# Compute mean of each feature per cluster (excluding noise points)\n",
    "valid_clusters = cluster_summary[cluster_summary['Cluster'] != -1]\n",
    "summary = valid_clusters.groupby('Cluster').mean()\n",
    "\n",
    "# Reverse MinMax scaling to restore original numerical values\n",
    "summary_original_scale = summary.copy()\n",
    "summary_original_scale[numerical_features.columns] = scaler.inverse_transform(summary[numerical_features.columns])\n",
    "\n",
    "# Restore categorical feature labels\n",
    "for cat_col in categorical_columns:\n",
    "    one_hot_columns = [col for col in summary_original_scale.columns if col.startswith(cat_col + \"_\")]\n",
    "    summary_original_scale[cat_col] = summary_original_scale[one_hot_columns].idxmax(axis=1).str.split('_').str[1]\n",
    "    summary_original_scale = summary_original_scale.drop(columns=one_hot_columns)\n",
    "\n",
    "# Display final cluster characteristics\n",
    "from IPython.display import display\n",
    "print(\"üîπ **Cluster Characteristics (Original Scale & Categorical Labels Restored):**\")\n",
    "display(summary_original_scale)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
